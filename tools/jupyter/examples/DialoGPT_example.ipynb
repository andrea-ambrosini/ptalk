{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb3b009d-cc41-4975-b418-14544dd64181",
   "metadata": {
    "tags": []
   },
   "source": [
    "[Original documentation](https://gradio.app/creating-a-chatbot/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81ec2d3-1cd4-4224-b07c-9e35b5756b71",
   "metadata": {},
   "source": [
    "[A State-of-the-Art Large-scale Pretrained Response generation model from huggingface.co](https://huggingface.co/microsoft/DialoGPT-medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5381676-6e5d-495d-b729-ae1b11b7e874",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 12:37:29.382371: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f374d6742c47d5a8e5559d07f09281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980d7ed4116a41a79a5d16ba40091f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138aa3dedfed42c2a98efd225f7f4a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e913aae23b2a4d8bb6cd522ab684a712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113a566356864bdb98dc3b4b4469984d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/863M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5031cab2c9cb4d788172f533fe6f7689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43f70637-4db2-42c1-8381-eb36975c48d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(input, history=[]):\n",
    "    # tokenize the new input sentence\n",
    "    new_user_input_ids = tokenizer.encode(input + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([torch.LongTensor(history), new_user_input_ids], dim=-1)\n",
    "\n",
    "    # generate a response \n",
    "    history = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id).tolist()\n",
    "\n",
    "    # convert the tokens to text, and then split the responses into lines\n",
    "    response = tokenizer.decode(history[0]).split(\"<|endoftext|>\")\n",
    "    response = [(response[i], response[i+1]) for i in range(0, len(response)-1, 2)]  # convert to tuples of list\n",
    "    return response, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ada0cca4-2916-496f-973f-3d631598e4b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "Running on public URL: https://a21a1c48-840e-4c1a.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a21a1c48-840e-4c1a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    state = gr.State([])\n",
    "\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\").style(container=False)\n",
    "\n",
    "    txt.submit(predict, [txt, state], [chatbot, state])\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
